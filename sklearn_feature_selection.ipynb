{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sklearn feature selection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sime1/notebooks/blob/master/sklearn_feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xoXvse-AHKh",
        "colab_type": "text"
      },
      "source": [
        "# Feature selection in sklearn\n",
        "\n",
        "In questo notebook provo ad utilizzare i diversi metodi di feature selection offerti da sklearn. Per applicare i metodi utilizzo il dataset `covertype` fornito da `sklearn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN7xEn88CKph",
        "colab_type": "code",
        "outputId": "3b2e5401-1ecf-4e34-d3af-344a79325123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_covtype\n",
        "\n",
        "ds = fetch_covtype()\n",
        "X, y = ds.data, ds.target\n",
        "print(ds.DESCR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _covtype_dataset:\n",
            "\n",
            "Forest covertypes\n",
            "-----------------\n",
            "\n",
            "The samples in this dataset correspond to 30×30m patches of forest in the US,\n",
            "collected for the task of predicting each patch's cover type,\n",
            "i.e. the dominant species of tree.\n",
            "There are seven covertypes, making this a multiclass classification problem.\n",
            "Each sample has 54 features, described on the\n",
            "`dataset's homepage <https://archive.ics.uci.edu/ml/datasets/Covertype>`__.\n",
            "Some of the features are boolean indicators,\n",
            "while others are discrete or continuous measurements.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    =================   ============\n",
            "    Classes                        7\n",
            "    Samples total             581012\n",
            "    Dimensionality                54\n",
            "    Features                     int\n",
            "    =================   ============\n",
            "\n",
            ":func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;\n",
            "it returns a dictionary-like object\n",
            "with the feature matrix in the ``data`` member\n",
            "and the target values in ``target``.\n",
            "The dataset will be downloaded from the web if necessary.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKnFjhWWAUzM",
        "colab_type": "text"
      },
      "source": [
        "## Panoramica\n",
        "\n",
        "Il processo di feature selection si occupa di ridurre il numero di variabili del dataset che il modello prende in considerazione, con lo scopo di migliorare l'accuracy del modello evitando l'overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzCAQWzMBLwN",
        "colab_type": "text"
      },
      "source": [
        "### `VarianceThreshold`\n",
        "\n",
        "`VarianceThreshold` è un *feature selector* che rimuove le features che non raggiungono una determinata varianza. Con i parametri di default, questo si traduce nella rimozione delle feature che hanno lo stesso valore per tutti i sample (varianza 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msIR56_dAEh2",
        "colab_type": "code",
        "outputId": "8a792555-403b-496a-9067-5ca0b040c0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.feature_selection import  VarianceThreshold\n",
        "\n",
        "sel = VarianceThreshold(threshold=0.01)\n",
        "sel.fit(X)\n",
        "filtered = sel.transform(X)\n",
        "\n",
        "print(X.shape)\n",
        "print(filtered.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(581012, 54)\n",
            "(581012, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs6B3rCoFc6x",
        "colab_type": "text"
      },
      "source": [
        "Come si può vedere, con questo metodo utilizzando un threshold di `0.01` il numero di feature viene ridotto da 54 a 33"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUWEqbYYFnz8",
        "colab_type": "text"
      },
      "source": [
        "### `SelectKBest` e `SelectPercentile`\n",
        "\n",
        "Qesti due feature selector permettono di selezionare le feature che hanno score più alto; `SelectKBest` seleziona un numero prefissato di feature, mentre `SelectPercentile` seleziona una percentuale delle feature.\n",
        "\n",
        "Entrambe le funzioni chiedono di specificare la funzione da utilizzare per calcolare lo score delle feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcxlpktNKCe6",
        "colab_type": "code",
        "outputId": "0b22e304-6ea9-45ea-9146-d18ef8bc45e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, SelectPercentile, mutual_info_classif\n",
        "\n",
        "sel_kbest = SelectKBest(score_func=mutual_info_classif,k=30)\n",
        "sel_percent = SelectPercentile(score_func=mutual_info_classif, percentile=60)\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "sel_kbest.fit(X[:1000], y[:1000])\n",
        "kbest_data = sel_kbest.transform(X)\n",
        "print(kbest_data.shape)\n",
        "\n",
        "sel_percent.fit(X[:1000], y[:1000])\n",
        "percent_data = sel_percent.transform(X)\n",
        "print(percent_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(581012, 54)\n",
            "(581012, 30)\n",
            "(581012, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hovKVeErOoPN",
        "colab_type": "text"
      },
      "source": [
        "In questo caso ho utilizzato un numero ridotto di sample, in quanto utilizzando l'intero dataset impiega molto tempo. Da notare comunque che questi metodi andrebbero applicati solo al set di training e non all'intero dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDfAVBcoPA-r",
        "colab_type": "text"
      },
      "source": [
        "### `RFE` e `RFECV`\n",
        "\n",
        "Viene utilizzato un predittore esterno che assegna pesi alle diverse feature. Viene effettuato il training; i pesi ottenuti durante il training vengono considerati come indicatore dell'importanza delle features, quindi viene effettuato il pruning delle features meno importanti. Questa procedura è ripetuta più volte, finchè non si raggiunge il numero desiderato di features.\n",
        "\n",
        "Nel caso di `REFCV` questo processo viene effettuato iterativamente, compiendo cross validation, per determinare il numero ottimale di features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM-Mi7kXSAc1",
        "colab_type": "code",
        "outputId": "92d9f27b-c54d-4c1a-c7c1-3c19164690c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from sklearn.feature_selection import RFE, RFECV\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "est = Lasso()\n",
        "\n",
        "sel_rfe = RFE(estimator=est, n_features_to_select=30)\n",
        "sel_rfecv = RFECV(estimator=est)\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "sel_rfe.fit(X[:10000], y[:10000])\n",
        "rfe_data = sel_rfe.transform(X)\n",
        "print(rfe_data.shape)\n",
        "\n",
        "sel_percent.fit(X[:10000], y[:10000])\n",
        "percent_data = sel_percent.transform(X)\n",
        "print(percent_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(581012, 54)\n",
            "(581012, 30)\n",
            "(581012, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK39nTPwTn5k",
        "colab_type": "text"
      },
      "source": [
        "Anche in questo caso ho utilizzato un numero ridotto di sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-1Y7kPCTzHo",
        "colab_type": "text"
      },
      "source": [
        "### `SelectFromModel`\n",
        "\n",
        "Anche questo metodo funziona utilizzando un predittore esterno e valutando l'importanza delle feature; in questo caso però invece di utilizzaro un procedimento ricorsivo, vengono utilizzati un valore di threshold oppure delle euristiche per la selezione delle features ritenute importanti.\n",
        "\n",
        "Le euristiche messe a disposizione da `sklearn` sono `mean`, `median` e loro multipli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4FUooMSUwrJ",
        "colab_type": "code",
        "outputId": "69c6b789-d022-482e-c532-bf7bbac87ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "est = DecisionTreeClassifier()\n",
        "sel = SelectFromModel(estimator=est, threshold='0.5*mean')\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "sel.fit(X, y)\n",
        "filtered = sel.transform(X)\n",
        "print(filtered.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(581012, 54)\n",
            "(581012, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFlVXZ-dWEtE",
        "colab_type": "text"
      },
      "source": [
        "In questo codice ho utilizzato un albero di decisione come classificatore, ed ho utilizzato il valore `'0.5*mean'` come threshold (ossia $1/2$ della media); il feature selector ha selezionato le 15 feature più importanti"
      ]
    }
  ]
}